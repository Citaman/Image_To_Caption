# -*- coding: utf-8 -*-
"""Image to Caption.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AfoHgf3pf84EzaKqdCk7wR3bm7cCdmJ4

# Image To Caption

## Download and load the Dataset (Flickr 30k)

### Install kaggle and dataset
"""

import os
os.environ['KAGGLE_USERNAME'] = "anthonnyolime"
os.environ['KAGGLE_KEY'] = "ac663c3e6c759875db943aedda32c95e"
!kaggle datasets download -d hsankesara/flickr-image-dataset

!unzip flickr-image-dataset.zip

"""### Start variable"""

import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('flickr30k_images/results.csv',sep='|')

df.head(3)

df_new =df.rename(columns={' comment':'comment',' comment_number':'comment_number'})

df_new.at[30,'comment']

def inspect_datsete(n):
  img =plt.imread('flickr30k_images/flickr30k_images/flickr30k_images/'+df_new.at[n,'image_name'])
  plt.title(df_new.at[n,'comment'])
  plt.imshow(img)

inspect_datsete(40)

len(df_new)

"""## Test model for text generation only"""

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
import keras
import numpy as np
import spacy
import string

#!python -m spacy download en_core_web_lg

import en_core_web_sm

nlp = en_core_web_sm.load()
#nlp = spacy.load("en_core_web_lg")
def most_similar_vec(word, count=10):
    by_similarity = sorted(word.vocab, key=lambda w: word.similarity(w), reverse=True)
    return [w.orth_ for w in by_similarity[:count]]

sentence = df_new['comment']
sentence.fillna(' ')

#table = str.maketrans('', '', string.punctuation)
dictionary = []
for e in sentence:
  try:
    a = [word.lower() for word in e.strip().split(' ') if word.isalpha()]
    dictionary +=a
  except:
    print(e)

len(dictionary)

dict_set = sorted(set(dictionary))

len(dict_set)

dict_set[:10]

''' TRreeees longgggg
count_word= []
for e in dict_set:
  count_word.append((e,dictionary.count(e)))
sort_count_word = sorted(count_word,key= lambda x : x[1],reverse=True)
sort_count_word[:1000]
'''

corrected_sent=[]
for i,e in enumerate(sentence):
  try:
    corrected_sent.append('<start> '+(' '.join([word.lower() for word in e.strip().split(' ') if word.isalpha()]))+' <end>')
    print(e)

len(corrected_sent)

#vocab_size = len(dict_set)+2
vocab_size = 5000

tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size,oov_token="<unk>",filters='!"#$%&()*+,-./:;=?@[\\]^_`{|}~\t\n',split=' ', char_level=False)
tokenizer.fit_on_texts(corrected_sent)

a = tokenizer.texts_to_sequences(corrected_sent)
len(a),int(len(a)/4)

from functools import reduce

max_length = len(reduce(lambda x,y : x if len(x)>len(y)else y,a))

x,y = [],[]
count =0
for e in a[:int(len(a)/pow(2,6))] :
  for i,_ in enumerate(e[1:]) :
    x.append(tf.keras.preprocessing.sequence.pad_sequences([e[:i+1]], maxlen=max_length)[0])
    y.append(tf.keras.utils.to_categorical(e[i+1],vocab_size)) 
    count+=1
print(count)
#len(x),len(y)

len(x),len(y)

from tensorflow.keras.layers import Input , LSTM , Embedding,Dense,Dropout
from tensorflow.keras.callbacks import LambdaCallback
from tensorflow.keras.models import Model
input = Input(shape=(max_length,))
embedding = Embedding(vocab_size,256,mask_zero=False)(input)
drop = Dropout(0.5)(embedding)
lstm = LSTM(256)(drop)
output = Dense(vocab_size,activation='softmax')(lstm)

model = Model(inputs=input, outputs=output)
model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])
tf.keras.utils.plot_model(model, "my_first_model.png",show_shapes=True,show_layer_names=False,dpi=150,expand_nested=False,rankdir='LR')

def test_model(batch,_):
  #_could_use_gpu_kernel = False
  intext ='<start>'
  for _ in range(max_length):
    test_seq = tokenizer.texts_to_sequences([intext])
    #print(test_seq)
    pad_test_seq = tf.keras.preprocessing.sequence.pad_sequences([test_seq],maxlen=max_length)[0]
    #print(pad_test_seq)
    res = model.predict(pad_test_seq)
    intext += ' '+tokenizer.index_word[res[-1].argmax()]
    #print([e.argmax() for e in res])
    if tokenizer.index_word[res[-1].argmax()] == '<end>':
        break
  print('\n',intext)

model.fit(np.array(x),np.array(y),batch_size=64,epochs=20,callbacks=[LambdaCallback(on_epoch_end=test_model)])

!nvidia-smi

test_model(None,None)

"""## Actual Model for image captionning (word)

###Preprocessing

####Import
"""

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
import numpy as np
import pickle
from keras.applications.vgg16 import VGG16
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing.image import load_img
from keras.preprocessing.image import load_img
from keras.preprocessing.image import img_to_array
from keras.applications.vgg16 import preprocess_input
import random

"""#### Variable"""

df_new

image_dataset = df_new['image_name']
image_dataset

comment_dataset = df_new['comment']
comment_dataset

uni_image = image_dataset.unique()

comment_dataset_train=[]
comment_dataset.replace(np.nan, '', regex=True)
for i,e in enumerate(comment_dataset):
  try:
    comment_dataset_train.append('<start> '+(' '.join([word.lower() for word in e.strip().split(' ') if word.isalpha() ]))+' <end>')
  except:
    print(e, type(e))
    comment_dataset_train.append('<start> '+'nan'+' <end>')
    continue

df_new['comment_train'] = comment_dataset_train
df_new

vocab_size = 5000

tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size,oov_token="<unk>",filters='!"#$%&()*+,-./:;=?@[\\]^_`{|}~\t\n',split=' ', char_level=False)
tokenizer.fit_on_texts(comment_dataset_train)

max_length = 80

vgg = VGG16()
vgg16_extract = Model(inputs=vgg.inputs, outputs=vgg.layers[-2].output)

"""#### Function"""

def load_img_from_path(img):
  path = 'flickr30k_images/flickr30k_images/flickr30k_images/'
  img = load_img(path+img,target_size=(224,224))
  im_arr= img_to_array(img)
  im_arr = im_arr.reshape((1, im_arr.shape[0], im_arr.shape[1], im_arr.shape[2]))
  im_arr = preprocess_input(im_arr)
  predict = vgg16_extract.predict(im_arr,verbose=0)
  #print(predict)
  return predict[0]

def from_comment_to_data(tokenizer,image,sentences,max_length,vocab_size):
  X_image, X_sequence, y = list(), list(), list()
  #print(sentences[0:1])
  for e in sentences:
    sequence = tokenizer.texts_to_sequences([e])
    for i,_ in enumerate(sequence[0][1:]):
      X_image.append(image)
      X_sequence.append(tf.keras.preprocessing.sequence.pad_sequences([sequence[0][:i+1]], maxlen=max_length)[0])
      y.append(tf.keras.utils.to_categorical(sequence[0][i+1],vocab_size))
  return np.array(X_image),np.array(X_sequence),np.array(y)

def data_generator(df_train,data_image_train,tokenizer,max_length,vocab_size):
  while 1:
    for image in data_image_train:
      sentences = df_train[df_train['image_name'] == image]["comment_train"]
      image_vgg = load_img_from_path(image)
      X_image, X_sequence, y = from_comment_to_data(tokenizer,image_vgg,sentences,max_length,vocab_size)
      yield ((X_sequence,X_image),y)

def inspect_datset_from_path(img_name):
  #print('flickr30k_images/flickr30k_images/flickr30k_images/'+img_name)
  img =plt.imread('flickr30k_images/flickr30k_images/flickr30k_images/'+img_name)
  print('\n',list(df_new[df_new['image_name'] == img_name]["comment_train"])[0:])
  plt.imshow(img)

def plot_probability_word(res):
    a = np.argsort(res[-1])[::-1]
    print(a[:10],res[-1][a][:10])
    print(tokenizer.index_word[a[0]])
    plt.figure(figsize=(25,10),dpi=40)
    plt.bar([i for i in range(10)], res[-1][a][:10])
    plt.xticks([i for i in range(10)],[tokenizer.index_word[e] for e in a[:10]],fontsize=30,rotation=30)
    plt.show()

def test_model(batch,logs):
  image = np.random.choice(uni_image,1)[0]
  intext ='<start>'
  img = load_img_from_path(image)
  image_input = np.array(img).reshape(1,-1)
  inspect_datset_from_path(image)
  for _ in range(10):
    test_seq = tokenizer.texts_to_sequences([intext])
    pad_test_seq = tf.keras.preprocessing.sequence.pad_sequences([test_seq],maxlen=max_length)[0]
    text = np.array(pad_test_seq).reshape(1,-1)
    
    res = model.predict((text,image_input),verbose=0)

    intext += ' '+tokenizer.index_word[res[-1].argmax()]
    #a = np.argsort(res[-1])[::-1]
    #ra = np.random.choice(a,p=res[0])
    #intext +=' '+tokenizer.index_word[ra]
    plot_probability_word(res)
    '''if tokenizer.index_word[ra] == '<end>':
        break'''
    if tokenizer.index_word[res[-1].argmax()] == '<end>':
        break
  print('\n',intext)

np.random.shuffle(uni_image)
generator = data_generator(df_new,uni_image, tokenizer, max_length, vocab_size)
inputs, outputs= next(generator)
print(inputs[0].shape)
print(inputs[1].shape)
print(outputs.shape)

"""#### Test"""

test_model(None,None)

#tokenizer.word_index

num = 6000
print(str(df_new.at[num,'comment_train']))
s= tokenizer.texts_to_sequences([df_new.at[num,'comment_train']])
m = tokenizer.sequences_to_texts(s)
print(m)

#inspect_datset_from_path(uni_image[0])

from google.colab import drive
drive.mount('/content/drive')

"""###Model"""

from tensorflow.keras.layers import Input , LSTM , Embedding,Dense,Dropout,add
from tensorflow.keras.callbacks import LambdaCallback
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import SGD

input1 = Input(shape=(max_length,))
embedding = Embedding(vocab_size,256,mask_zero=False)(input1)
drop = Dropout(0.5)(embedding)
lstm = LSTM(256)(drop)

input2 = Input(shape=(4096,))
drop2 = Dropout(0.5)(input2)
dense = Dense(256, activation='relu')(drop2)

decode = add([dense,lstm])
decode = Dense(256,activation='relu')(decode)

output = Dense(vocab_size,activation='softmax')(decode)

model = Model(inputs=[input1,input2], outputs=output)
model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])
model.summary()
tf.keras.utils.plot_model(model, "my_first_model.png",show_shapes=True,show_layer_names=False,dpi=150,expand_nested=False,rankdir='LR')

model.fit(generator,steps_per_epoch=100,epochs = 100,callbacks=[LambdaCallback(on_epoch_end=test_model)])

"""## Actual Model for image captionning (character)

### Import
"""

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
import numpy as np
import pickle
from keras.applications.vgg16 import VGG16
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing.image import load_img
from keras.preprocessing.image import load_img
from keras.preprocessing.image import img_to_array
from keras.applications.vgg16 import preprocess_input
import random

"""### Variable"""

df_new

image_dataset = df_new['image_name']
image_dataset

comment_dataset = df_new['comment']
comment_dataset

uni_image = image_dataset.unique()

comment_dataset_train_character=[]
comment_dataset.replace(np.nan, '', regex=True)
for i,e in enumerate(comment_dataset):
  try:
    comment_dataset_train_character.append('> '+(' '.join([word.lower() for word in e.strip().split(' ') if word.isalpha()]))+' <')
  except:
    print(e, type(e))
    comment_dataset_train_character.append('> '+'nan'+' <')
    continue

df_new['comment_train'] = comment_dataset_train_character
df_new

vocab_size = 31

tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size,oov_token="<unk>",filters='!"#$%&()*+,-./:;=?@[\\]^_`{|}~\t\n',split=' ', char_level=True)
tokenizer.fit_on_texts(comment_dataset_train_character)

tokenizer.word_index

from functools import reduce

max_length = len(reduce(lambda x,y : x if len(x)>len(y)else y,df_new['comment_train']))

max_length

vgg = VGG16()
vgg16_extract = Model(inputs=vgg.inputs, outputs=vgg.layers[-2].output)

num = 0
print(str(df_new.at[num,'comment_train']))
s= tokenizer.texts_to_sequences([df_new.at[num,'comment_train']])
m = tokenizer.sequences_to_texts(s)
print(len(s[0]))

"""### Function"""

def load_img_from_path(img):
  path = 'flickr30k_images/flickr30k_images/flickr30k_images/'
  img = load_img(path+img,target_size=(224,224))
  im_arr= img_to_array(img)
  im_arr = im_arr.reshape((1, im_arr.shape[0], im_arr.shape[1], im_arr.shape[2]))
  im_arr = preprocess_input(im_arr)
  predict = vgg16_extract.predict(im_arr,verbose=0)
  #print(predict)
  return predict[0]

def from_comment_to_data(tokenizer,image,sentences,max_length,vocab_size):
  X_image, X_sequence, y = list(), list(), list()
  for e in sentences:
    sequence = tokenizer.texts_to_sequences([e])
    for i,_ in enumerate(sequence[0][1:]):
      X_image.append(image)
      X_sequence.append(tf.keras.preprocessing.sequence.pad_sequences([sequence[0][:i+1]], maxlen=max_length)[0])
      y.append(tf.keras.utils.to_categorical(sequence[0][i+1],vocab_size))
  return np.array(X_image),np.array(X_sequence),np.array(y)

def data_generator(df_train,data_image_train,tokenizer,max_length,vocab_size):
  while 1:
    for image in data_image_train:
      sentences = df_train[df_train['image_name'] == image]["comment_train"]
      image_vgg = load_img_from_path(image)
      X_image, X_sequence, y = from_comment_to_data(tokenizer,image_vgg,sentences,max_length,vocab_size)
      yield ((X_sequence,X_image),y)

def inspect_datset_from_path(img_name):
  #print('flickr30k_images/flickr30k_images/flickr30k_images/'+img_name)
  img =plt.imread('flickr30k_images/flickr30k_images/flickr30k_images/'+img_name)
  print('\n',list(df_new[df_new['image_name'] == img_name]["comment_train"])[0:])
  plt.imshow(img)

def plot_probability_word(res):
    a = np.argsort(res[-1])[::-1]
    print(a[:10],res[-1][a][:10])
    print(tokenizer.index_word[a[0]])
    plt.figure(figsize=(25,10),dpi=40)
    plt.bar([i for i in range(10)], res[-1][a][:10])
    plt.xticks([i for i in range(10)],[tokenizer.index_word[e] for e in a[:10]],fontsize=30,rotation=30)
    plt.show()

def test_model(batch,logs):
  image = np.random.choice(uni_image,1)[0]
  intext ='>'
  img = load_img_from_path(image)
  image_input = np.array(img).reshape(1,-1)
  inspect_datset_from_path(image)
  for _ in range(30):
    test_seq = tokenizer.texts_to_sequences([intext])
    pad_test_seq = tf.keras.preprocessing.sequence.pad_sequences([test_seq],maxlen=max_length)[0]
    text = np.array(pad_test_seq).reshape(1,-1)
    
    res = model.predict((text,image_input),verbose=0)

    intext += ' '+tokenizer.index_word[res[-1].argmax()]
    plot_probability_word(res)
    if tokenizer.index_word[res[-1].argmax()] == '<':
        break
  print('\n',intext)

np.random.shuffle(uni_image)
generator = data_generator(df_new,uni_image, tokenizer, max_length, vocab_size)
inputs, outputs= next(generator)
inputs, outputs= next(generator)
inputs, outputs= next(generator)
print(inputs[0].shape)
print(inputs[1].shape)
print(outputs.shape)

"""### Model"""

from tensorflow.keras.layers import Input , LSTM , Embedding,Dense,Dropout,add
from tensorflow.keras.callbacks import LambdaCallback
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import SGD

input1 = Input(shape=(max_length,))
embedding = Embedding(vocab_size,256,mask_zero=False)(input1)
drop = Dropout(0.5)(embedding)
lstm = LSTM(256)(drop)

input2 = Input(shape=(4096,))
drop2 = Dropout(0.5)(input2)
dense = Dense(256, activation='relu')(drop2)

decode = add([dense,lstm])
decode = Dense(256,activation='relu')(decode)

output = Dense(vocab_size,activation='softmax')(decode)

model = Model(inputs=[input1,input2], outputs=output)
model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])
model.summary()
tf.keras.utils.plot_model(model, "my_first_model.png",show_shapes=True,show_layer_names=False,dpi=150,expand_nested=False,rankdir='LR')

model.fit(generator,steps_per_epoch=10,epochs = 100,callbacks=[LambdaCallback(on_epoch_end=test_model)])



"""## Image to Caption with Attention"""

